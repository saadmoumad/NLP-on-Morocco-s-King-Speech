{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_based.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaWN7FHAekxX",
        "colab_type": "code",
        "outputId": "0e975176-9ad9-4dd9-f14b-1aacb1e5b8bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''from google.colab import drive\n",
        "drive.mount('/content/gdrive')'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCMDvUCvqhlF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLQ4-H_gMY_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/saadmoumad/NLP-on-Morocco-s-King-Speech"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_II4lSJ3MZG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Scraping Discours from maroc.ma \n",
        "!python3 NLP-on-Morocco-s-King-Speech/DiscourScraping.py\n",
        "#Alredy pushed with git-repo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9cc3MJZGdBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukthk_qLF1S8",
        "colab_type": "code",
        "outputId": "dbaa78e2-1970-4238-d4e1-00f85e456a20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "#installing Spacy for text preprocessing\n",
        "!pip3 install spacy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (47.1.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.4.5.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hETrNJaB0XNa",
        "colab_type": "code",
        "outputId": "5fc7666e-373f-47b0-fed1-788a31a814a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "#Downloading French model\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fr_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz#egg=fr_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (47.1.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L17delzxKGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load(\"/usr/local/lib/python3.6/dist-packages/fr_core_news_sm/fr_core_news_sm-2.2.5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSLNY5Y7xbAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = 'NLP-on-Morocco-s-King-Speech/Disc_Text'\n",
        "files_name = os.listdir(data_dir)\n",
        "files_name.sort()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7eYdviJ3LGI",
        "colab_type": "code",
        "outputId": "e91d4a0a-c26d-4a70-f3fd-23a16b9a9890",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(files_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "205"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW0YGjCCP6EN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Passing a Spacy.doc object and returning a list of string Token \n",
        "def get_token_list(doc):\n",
        "    l = []\n",
        "    for token in doc:\n",
        "        l.append(str(token))\n",
        "    return l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNdbDPNFg7ph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Return a set objet of unique token in all 'Discours'\n",
        "def get_vocab_set(files_name):\n",
        "    s = set()\n",
        "    for file in files_name[1:]: #exclu .DS_Store file\n",
        "        text = open(os.path.join(data_dir, file), 'rb').read().decode(encoding='utf-8')\n",
        "        doc = nlp(text)\n",
        "        token_list = get_token_list(doc)\n",
        "        s_next = set(token_list)\n",
        "        s = s.union(s_next.difference(s))\n",
        "    return sorted(s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgtasbKfg-3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_set = get_vocab_set(files_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-mJieJqhhpM",
        "colab_type": "code",
        "outputId": "f888b561-7898-40bb-c7ca-2ff127fb9dcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(vocab_set)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15701"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QROQou-RioQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert Text (as Tokens) to int\n",
        "def text_as_int(files_name, word2idx):\n",
        "    text_int = []\n",
        "    for file in files_name[1:]: #exclu .DS_Store file\n",
        "          text = open(os.path.join(data_dir, file), 'rb').read().decode(encoding='utf-8')\n",
        "          doc = nlp(text)\n",
        "          token_list = get_token_list(doc)\n",
        "          text_int += [word2idx[word] for word in token_list]\n",
        "    return text_int"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKne4K84hBaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique Token to indices\n",
        "word2idx = {u:i for i, u in enumerate(vocab_set)}\n",
        "idx2word = np.array(vocab_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuS8lvRHj75f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_as_int = np.array(text_as_int(files_name, word2idx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaaGTMJ2j82-",
        "colab_type": "code",
        "outputId": "1d20f405-54cb-4a3a-d6d3-51ceab065e32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(text_as_int)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "447294"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NmyPcBtlUNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 30\n",
        "examples_per_epoch = len(text_as_int)//(seq_length+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSHNRMPlmEI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4A0fQn7mHtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = words_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boYp6MabmS3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_8Bode8ml5r",
        "colab_type": "code",
        "outputId": "156f857a-2e51-4152-b4fe-c74ec8b47296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "for i in dataset.take(2):\n",
        "  print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(30,), dtype=int64, numpy=\n",
            "array([   13,  1368, 14926,   874,    13,  1809,  8944,  1758,  6738,\n",
            "        8944,   653, 13369, 13808,  9000,  1768,    26,  1928,  1026,\n",
            "        6738,  1982,   766, 10756, 14926,  8944, 14193,  8464, 10407,\n",
            "        9984, 14859, 11677])>, <tf.Tensor: shape=(30,), dtype=int64, numpy=\n",
            "array([ 1368, 14926,   874,    13,  1809,  8944,  1758,  6738,  8944,\n",
            "         653, 13369, 13808,  9000,  1768,    26,  1928,  1026,  6738,\n",
            "        1982,   766, 10756, 14926,  8944, 14193,  8464, 10407,  9984,\n",
            "       14859, 11677,    26])>)\n",
            "(<tf.Tensor: shape=(30,), dtype=int64, numpy=\n",
            "array([ 1928,  1419,  9000,  1873,  1155,  1189,    26, 11787,   874,\n",
            "        9215,  6583,  5320, 13413,  8295,  1495,    26, 11018,  4414,\n",
            "        9000,  8969,  5320,  8944,  7547,  1435,  2191,    26,  9997,\n",
            "       13410,  7751,  5320])>, <tf.Tensor: shape=(30,), dtype=int64, numpy=\n",
            "array([ 1419,  9000,  1873,  1155,  1189,    26, 11787,   874,  9215,\n",
            "        6583,  5320, 13413,  8295,  1495,    26, 11018,  4414,  9000,\n",
            "        8969,  5320,  8944,  7547,  1435,  2191,    26,  9997, 13410,\n",
            "        7751,  5320, 14419])>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgatnrBtmxNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMclr9v2m8B1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the Tokens vocab\n",
        "vocab_size = len(vocab_set)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2IA-HyHnOEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hAyjOePnUsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab_set),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB922EZ5nW2t",
        "colab_type": "code",
        "outputId": "320dceb7-1d95-4348-e97c-656baa86bea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (32, None, 256)           4019456   \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (32, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (32, None, 15701)         16093525  \n",
            "=================================================================\n",
            "Total params: 24,051,285\n",
            "Trainable params: 24,051,285\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAVtIDmWncmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t68xAFddnjwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Checkpoints for model training\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "islDGonbnmt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvDfkEgnqEU",
        "colab_type": "code",
        "outputId": "fcac7bc8-66fe-4147-cc21-8755ad7d0335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "#fit Data to model\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "450/450 [==============================] - 53s 119ms/step - loss: 6.2119\n",
            "Epoch 2/20\n",
            "450/450 [==============================] - 53s 117ms/step - loss: 5.2796\n",
            "Epoch 3/20\n",
            "450/450 [==============================] - 53s 118ms/step - loss: 4.8994\n",
            "Epoch 4/20\n",
            "450/450 [==============================] - 53s 118ms/step - loss: 4.2236\n",
            "Epoch 5/20\n",
            "450/450 [==============================] - 54s 119ms/step - loss: 3.7160\n",
            "Epoch 6/20\n",
            "450/450 [==============================] - 53s 118ms/step - loss: 3.2289\n",
            "Epoch 7/20\n",
            "450/450 [==============================] - 53s 118ms/step - loss: 2.7709\n",
            "Epoch 8/20\n",
            "450/450 [==============================] - 53s 119ms/step - loss: 2.3841\n",
            "Epoch 9/20\n",
            "450/450 [==============================] - 53s 118ms/step - loss: 2.0789\n",
            "Epoch 10/20\n",
            "450/450 [==============================] - 53s 117ms/step - loss: 1.8388\n",
            "Epoch 11/20\n",
            "450/450 [==============================] - 53s 117ms/step - loss: 1.6509\n",
            "Epoch 12/20\n",
            "450/450 [==============================] - 53s 118ms/step - loss: 1.5007\n",
            "Epoch 13/20\n",
            "450/450 [==============================] - 53s 119ms/step - loss: 1.3842\n",
            "Epoch 14/20\n",
            "450/450 [==============================] - 53s 118ms/step - loss: 1.2863\n",
            "Epoch 15/20\n",
            "450/450 [==============================] - 53s 118ms/step - loss: 1.2112\n",
            "Epoch 16/20\n",
            "450/450 [==============================] - 53s 119ms/step - loss: 1.1478\n",
            "Epoch 17/20\n",
            "450/450 [==============================] - 54s 119ms/step - loss: 1.0934\n",
            "Epoch 18/20\n",
            "450/450 [==============================] - 53s 118ms/step - loss: 1.0499\n",
            "Epoch 19/20\n",
            "450/450 [==============================] - 53s 118ms/step - loss: 1.0125\n",
            "Epoch 20/20\n",
            "450/450 [==============================] - 53s 119ms/step - loss: 0.9815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nrk2OdeECST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Rebuild with diff batch size and restore the weights from the latest checkpoint.\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umtz3SikEK05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  doc = nlp(start_string)\n",
        "  start_token = get_token_list(doc)\n",
        "  num_generate = 1000\n",
        "\n",
        "  input_eval = [word2idx[s] for s in start_token]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  text_generated = []\n",
        "  temperature = 1.0\n",
        "  model.reset_states()\n",
        "  \n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "      # using a categorical distribution to predict the token returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "      text_generated.append(idx2word[predicted_id])\n",
        "\n",
        "  return (start_string + ' '.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8GDnJSZFGje",
        "colab_type": "code",
        "outputId": "c4af093a-f1d5-4804-ed62-a86d8514f9d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"SM le Roi\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SM le RoiMohammed VI , que Dieu L' assiste , a adressÃ© samedi un discours Ã  la Nation Ã  lâ€™ occasion de la FÃªte du TrÃ´ne , qui coÃ¯ncide cette annÃ©e avec le 17-Ã¨me pour exprimer mes sincÃ¨res remerciements et ma considÃ©ration Ã  lâ€™ organisation et ce faire en son sein de l' environnement et du dÃ©veloppement , nous nous pouvons en nous engageons ensemble et dans le cadre de l' Initiative que Nous source de production , en prenant les dÃ©lais les plus ouvertes . \n",
            " C' est pourquoi Nous ne cessons d' exprimer , des nÃ©gociations qui ont Ã©tÃ© adoptÃ©es . \n",
            " Nous exprimons Ã©galement ces fondations Ã  la concertation , aux organisations de la sociÃ©tÃ© civile , sous lâ€™ impulsion du secteur public dans notre pays . A ce titre , Nous est dÃ©terminÃ© Ã  renforcer et Ã  veiller rÃ©solument sur la rÃ©gionalisation avancÃ©e , dans la mesure oÃ¹ les progrÃ¨s sont reconnus au voisinage sont un cadre pour les doter d' entre eux , et le soutien public et le dÃ©veloppement social de notre pays . \n",
            " Mes frÃ¨res Chefs dâ€™ Etat auront lâ€™ occasion aujourdâ€™hui aujourdâ€™hui , aujourdâ€™hui , de nous inspirer de vie active et l' immunisation du paysage euro - mÃ©diterranÃ©en . \n",
            " Nous rÃ©affirmons que la mise en place de l' Agence s' est ici contredit les manÅ“uvres Ã  de servir le citoyen , en veillant Ã  ce que les prÃ©judices du passÃ© soient rÃ©parÃ©s et les blessures pansÃ©es . Nous adopterons , Ã  cette fin , Nous invitons le gouvernement Ã  Ã©tablir un plan , voire pour cette rÃ©forme . \n",
            " Dans la mesure oÃ¹ l' on ne peut concevoir les objectifs en dÃ©veloppement Ã©conomique et sociale , qui ont besoin aux sacrifices que par les Etats maghrÃ©bins , l' Ã©laboration d' un vÃ©ritable sens dÃ©mocratique d' une rÃ©gionalisation naissante Ã  lâ€™ Afrique mÃ©rite par le Chef de lâ€™ ouverture qu' Mon regrettÃ© PÃ¨re , feu Leurs MajestÃ©s les Rois Mohammed V et Hassan II , bÃ©nie soit leur Ã¢me . \n",
            " Nous assurons de toute Notre considÃ©ration et Notre sollicitude , dont les Marocains sont autant investis , les autoritÃ©s qui ont peu que vous qui vise Ã  mettre au service de nos nations , et pour que les ambitions affichÃ©es sont lÃ©gitimes , nos provinces du sud et les fidÃ¨les sujets qui nâ€™ doivent pas perdre de tension . \n",
            " Il sâ€™ agit de cette tribune cette initiative du baccalaurÃ©at est une plus grande crÃ©dibilitÃ© de notre modÃ¨le de dÃ©veloppement Ã©conomique et social soutenu aussi bien voulu Me rÃ©server , MajestÃ© , de tout mettre sur la pauvretÃ© et l' exclusion . Le terrorisme n' est pas plus de raison pour mettre un terme Ã  leur occupation ? \n",
            " All rights reserved 2020 \n",
            " \" Louange Ã  DieuPriÃ¨re et Salut sur le ProphÃ¨te , Sa famille et ses Compagnons . \n",
            " MajestÃ©s , Excellences , Altesses , \n",
            " Nous en avons Ã©galement un changement auquel le tourisme des moyens de la Fondation Mohammed V pour la poursuite de la loi et les magistrats , confiance dans l' exercice de son exÃ©cution et de larges consultations sur les deux Chambres du parlement . Mais Nous pourrons apporter , tout en Notre qualitÃ© de PrÃ©sident du Groupe des 77 , plus que , tout comme enseignants Notre Organisation intervient dans la force que traversent les grandes orientations que le Maroc , imprÃ©gnÃ© de sa souverainetÃ© et de son refus unanime , et pour l' essentiel d' un effort collectif dans le domaine des entreprises . \n",
            " Axe IV : L' importance du combat Royal : \" gestion gÃ©nÃ©rale reste des projets quâ€™ elle induit et qui Nous est propre Ã  dire , a mÃªme acte que Nous puis en la ConfÃ©rence . Bali , pourront se faire une nation et animÃ©e plus lorsquâ€™ ensemble du peuple palestinien , sur ses droits et ses devoirs , Ã  s' sens et de toute la lettre de Notre concept et de la justice pour la paix et le dÃ©veloppement , jouissant de la lÃ©galitÃ© internationale , tant en initiant des prestations bancaires et dâ€™ identifier les faiblesses qui les Marocains . Nous avons veillÃ© Ã  ce que cette rÃ©union se fait dans sa politique comme un pays avec le Royaume . Il convient aussi d' accÃ©lÃ©rer l' Ã©laboration d' une vÃ©ritable rupture bilatÃ©rales paysage politique autour d' Ã©lections libres et nationaux , dans le cadre de l' unanimitÃ© nationale sans faille autour de la question , qu' il s' agit , dans une composition et un environnement libre , oÃ¹ la solidaritÃ© constante , avec la mÃªme dÃ©termination , le Maroc appelle de ses voeux un succÃ¨s de ce traitement de procÃ©der Ã  une nouvelle conscience onusienne aux Nations Unies , Ã  notre peuple reprÃ©sentatives , d' unie , de corps ou de terrorisme , unies , entre autres femmes qui persistent Ã  toute discrimination , son Premier Serviteur Ã©tait Prince HÃ©ritier , et eu Ã©gard aux Ã©lÃ©ments fondamentaux qui l' unissent , au mÃ©pris du projet de valeureux lui confÃ¨re Ã  notre peuple fier dâ€™ assumer pleinement ses responsabilitÃ©s , Ã  commencer par la crÃ©dibilitÃ© , grÃ¢ce Ã  la rÃ©ussite qui est sans la patrie des interventions , ainsi que nâ€™ est pas traitÃ© de droit Ã©conomique . Cette situation se doit de rÃ©pondre aux exigences de les peuples du Sud de la MÃ©diterranÃ©e , ainsi que , de faÃ§on utile et crÃ©dible , nos peuples arabes qui persistent Ã  la crÃ©ation de cette occasion , je tiens Ã  souligner que la sÃ©curitÃ© et la stabilitÃ© du monde sont menacÃ©es . Il se doit Ã©galement faire avancer rÃ©solument les traduire des comptes quâ€™ ils n' attachent nullement commune alors que persÃ©vÃ©rer sur ces provinces qui vivent la rÃ©habilitation de l' AmazighitÃ© et de Notre politique . Vous devez , donc , assumer , une nouvelle occasion , qui est sans cesse renouvelÃ©e , le Maroc et la plÃ©nitude de ses droits lÃ©gitimes , il est\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIGdIN1XFMNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}